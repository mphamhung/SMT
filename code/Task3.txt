---- Log probabilities with a sentence ----

SENTSTART It is indeed a great honour to be entrusted with this task . SENTEND

Delta: 0 log probability = -69.50919931145674
Delta: 0.1 log probability = -96.624558452306
Delta: 0.25 log probability = -104.55242946894558
Delta: 0.5 log probability = -111.50152771910308
Delta: 0.75 log probability = -115.96113259484274


SENTSTART Chers collegues , vous me faites un grand honneur en me confiant cette tache . SENTEND

Delta: 0 log probability = -102.78295021916664
Delta: 0.1 log probability = -143.96247442372575
Delta: 0.25 log probability = -156.39635131249753
Delta: 0.5 log probability = -166.4737523490347
Delta: 0.75 log probability = -172.47801535943339

Discussion:

We notice that the MLE model has the lowest perplexity and as we increase the delta smoothing, the perplexity also increases. Mathematically, we notice that the log likelihood decreases as delta increases, and the perplixity is inversly proportional to the log likelihood, thus explaining the increases in perplexity. From the below results we can conclude that increasing delta towards 1 results in a worse performing model. 
Refering to the test and training corpus, they are both from the Canadian Hansards, so their is potential for a large overlap in language. Adding delta smoothing to unknown words will thus not help that much.

---------
English Perplexity Testing

No delta: 11.565966234268274
delta 0.1: 26.072940267119247
delta 0.25: 37.141496010715876
delta 0.5: 51.35562204311967
delta 0.75: 63.518325812590504

French Perplexity Testing

No delta: 11.220180676791642
delta 0.1: 28.28387970745656
delta 0.25: 41.54750117518776
delta 0.5: 58.914053731811215
delta 0.75: 63.518325812590504
